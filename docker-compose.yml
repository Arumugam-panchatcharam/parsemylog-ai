version: '3.9'

services:
  rdk-logai-app:
    build: .
    image: logai-app:1.0
    container_name: rdk-logai-app
    command: ["gunicorn", "-w", "8", "-b", "0.0.0.0:40901", "--timeout", "360", "logai_wsgi:server"]
    volumes:
      - .:/app
    ports:
      - "${APP_PORT}:${APP_PORT}"
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=${CELERY_BROKER_URL}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND}
      - LLAMA_API_URL=${LLAMA_API_URL}
    depends_on:
      - redis
      - llama-service
    restart: always
    env_file:
      - .env

  celery-worker:
    build: ./services/celery_worker
    image: logai-app:1.0
    container_name: logai-celery-worker
    command: ["celery", "-A", "celery_app", "worker", "--loglevel=INFO"]
    environment:
      - CELERY_BROKER_URL=${CELERY_BROKER_URL}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND}
      - LLAMA_API_URL=${LLAMA_API_URL}
    depends_on:
      - redis
      - llama-service
    volumes:
      - ./services/celery_worker:/app
    restart: always
    env_file:
      - .env

  redis:
    image: redis:7-alpine
    container_name: logai-redis
    ports:
      - "${REDIS_PORT}:6379"
    restart: always
    env_file:
      - .env

  llama-service:
    build: ./services/llama
    image: llama-service:1.0
    container_name: logai-llama-service
    volumes:
      - ./services/llama:/app
      - ./ollama_data:/root/.ollama
      - /var/run/ollama.sock:/var/run/ollama.sock
    environment:
      - OLLAMA_MODEL=${OLLAMA_MODEL}
    ports:
      - "${LLAMA_HOST_PORT}:8000"
    restart: always
    env_file:
      - .env

  nginx:
    image: nginx:alpine
    container_name: nginx
    volumes:
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf:ro
    ports:
      - "${NGINX_PORT}:80"
    depends_on:
      - rdk-logai-app
    restart: always
    env_file:
      - .env